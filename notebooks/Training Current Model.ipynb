{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/vulgrim/miniconda3/envs/tensorflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: /home/vulgrim/miniconda3/envs/tensorflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: /home/vulgrim/miniconda3/envs/tensorflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: /home/vulgrim/miniconda3/envs/tensorflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: /home/vulgrim/miniconda3/envs/tensorflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: /home/vulgrim/miniconda3/envs/tensorflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: /home/vulgrim/miniconda3/envs/tensorflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "/bin/bash: /home/vulgrim/miniconda3/envs/tensorflow/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy -q\n",
    "%pip install keras -q\n",
    "\n",
    "%pip install tensorflow -q\n",
    "%pip install tensorboard_plugin_profile -q\n",
    "\n",
    "%pip install networkx -q\n",
    "\n",
    "%pip install wandb -q\n",
    "%pip install kaggle -q\n",
    "\n",
    "%pip install matplotlib -q\n",
    "%pip install ipympl -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path    \\\n",
    "    import      \\\n",
    "    isdir,      \\\n",
    "    join\n",
    "\n",
    "from os         \\\n",
    "    import      \\\n",
    "    walk,       \\\n",
    "    remove\n",
    "\n",
    "from random                     \\\n",
    "    import                      \\\n",
    "    SystemRandom\n",
    "\n",
    "import wandb\n",
    "\n",
    "from wandb.keras                \\\n",
    "    import                      \\\n",
    "    WandbMetricsLogger,         \\\n",
    "    WandbCallback,              \\\n",
    "    WandbEvalCallback\n",
    "\n",
    "from keras.utils                \\\n",
    "    import                      \\\n",
    "    image_dataset_from_directory\n",
    "\n",
    "from keras.backend              \\\n",
    "    import                      \\\n",
    "    clear_session\n",
    "\n",
    "from keras.models               \\\n",
    "    import                      \\\n",
    "    Sequential,                 \\\n",
    "    load_model\n",
    "\n",
    "from keras.layers               \\\n",
    "    import                      \\\n",
    "    RandomFlip,                 \\\n",
    "    RandomZoom,                 \\\n",
    "    RandomContrast,             \\\n",
    "    RandomBrightness,           \\\n",
    "    RandomRotation\n",
    "\n",
    "from keras.optimizers           \\\n",
    "    import SGD\n",
    "\n",
    "from keras.callbacks            \\\n",
    "    import TensorBoard\n",
    "\n",
    "from keras                      \\\n",
    "    import mixed_precision\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.python.ops      \\\n",
    "  import summary_ops_v2\n",
    "\n",
    "from keras.losses \\\n",
    "    import SparseCategoricalCrossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 326689), started 0:00:19 ago. (Use '!kill 326689' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7e8d516ab580aca0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7e8d516ab580aca0\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "is_using_tensorboard: bool = True\n",
    "location_to_tensorboard: str = '/tmp/tensorboard'\n",
    "\n",
    "if is_using_tensorboard:\n",
    "    if isdir(\n",
    "        location_to_tensorboard\n",
    "    ):\n",
    "        for root,           \\\n",
    "            directories,    \\\n",
    "            files           \\\n",
    "                in walk(\n",
    "                    location_to_tensorboard\n",
    "                ):\n",
    "            \n",
    "            for file in files:\n",
    "                full_path_to_file: str = join(\n",
    "                    root, \n",
    "                    file\n",
    "                )\n",
    "\n",
    "                remove(\n",
    "                    full_path_to_file\n",
    "                )\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {location_to_tensorboard}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ipympl: bool = False\n",
    "\n",
    "if use_ipympl:\n",
    "    %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_policy = mixed_precision.Policy(\n",
    "    'bfloat16'\n",
    ")\n",
    "\n",
    "mixed_precision.set_global_policy(\n",
    "    global_policy\n",
    ")\n",
    "\n",
    "tensorflow.keras                                        \\\n",
    "    .mixed_precision                                    \\\n",
    "    .set_global_policy(\n",
    "    global_policy\n",
    ")\n",
    "\n",
    "gpu_memory_growth: bool = True\n",
    "\n",
    "def zero() -> int:\n",
    "    return 0\n",
    "\n",
    "\n",
    "physical_devices = tensorflow.config.list_physical_devices(\n",
    "    str(\n",
    "        'gpu'\n",
    "    ).upper()\n",
    ")\n",
    "\n",
    "selected_physical_device = physical_devices[\n",
    "    zero()\n",
    "]\n",
    "\n",
    "tensorflow.config.experimental.set_memory_growth(\n",
    "    selected_physical_device,\n",
    "    gpu_memory_growth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_of_model: str = '/opt/models/O2RM'\n",
    "\n",
    "def get_location_of_model() -> str:\n",
    "    global location_of_model\n",
    "    return location_of_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_old_model_weights():\n",
    "    global          \\\n",
    "        model,      \\\n",
    "        location_of_model\n",
    "\n",
    "    if isdir(\n",
    "        location_of_model\n",
    "    ):\n",
    "        model.load_weights(\n",
    "            location_of_model\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "model = load_model(\n",
    "    location_of_model\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=SGD(\n",
    "        learning_rate=0.024\n",
    "    ),\n",
    "    loss=SparseCategoricalCrossentropy(\n",
    "        from_logits=True\n",
    "    ),\n",
    "    metrics=[\n",
    "        'accuracy'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seed():\n",
    "    return SystemRandom().randint(\n",
    "        1,\n",
    "        32767\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_seed: int = generate_seed()\n",
    "\n",
    "location_of_dataset: str = '/opt/dataset/numbers'\n",
    "\n",
    "width: int = 512\n",
    "height: int = 512\n",
    "channels: int = 3\n",
    "\n",
    "number_of_labels: int = 10\n",
    "\n",
    "batches: int = 50\n",
    "epochs: int = 12\n",
    "\n",
    "validation_split: float = 0.5\n",
    "\n",
    "use_multiprocessing: bool = True\n",
    "process_workers: int = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_entity: str = 'designermadsen'\n",
    "wandb_project: str = 'O2RM'\n",
    "\n",
    "wandb_jobtype: str = 'Training'\n",
    "\n",
    "wandb_tags: list = [\n",
    "    'Nvidia',\n",
    "    'Linux',\n",
    "    'Ubuntu',\n",
    "    'Development',\n",
    "    'Random',\n",
    "    'test-driven',\n",
    "    'Bare-Metal',\n",
    "    'TensorFlow'   \n",
    "]\n",
    "\n",
    "wandb_use_callback: bool = False\n",
    "wandb_measure_metrics: bool = True\n",
    "wandb_compute_flops: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history: list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration: dict = {\n",
    "    'vision': \n",
    "    {\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'channels': channels\n",
    "    },\n",
    "\n",
    "    'dataset': \n",
    "    {\n",
    "        'number of labels': number_of_labels,\n",
    "        'batches': batches,\n",
    "        'epochs': epochs,\n",
    "        'seed': dataset_seed,\n",
    "        'using multiprocessing': use_multiprocessing,\n",
    "        'using tensorboard': is_using_tensorboard,\n",
    "        'workers': process_workers,\n",
    "    },\n",
    "\n",
    "    'wandb':\n",
    "    {\n",
    "        'job type': wandb_jobtype,\n",
    "        'tags': wandb_tags,\n",
    "        'measure metrics': wandb_measure_metrics,\n",
    "        'use callbacks': wandb_use_callback,\n",
    "        'compute flops': wandb_compute_flops\n",
    "    },\n",
    "\n",
    "    'gpu':\n",
    "    {\n",
    "        'memory growth': gpu_memory_growth\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = None\n",
    "validation_dataset = None\n",
    "\n",
    "def get_training_dataset():\n",
    "    global training_dataset\n",
    "    return training_dataset\n",
    "\n",
    "def get_validation_dataset():\n",
    "    global validation_dataset\n",
    "    return validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_wandb_callback():\n",
    "    global wandb_compute_flops\n",
    "\n",
    "    return WandbCallback(\n",
    "        monitor='val_loss', \n",
    "        verbose=0,\n",
    "    \n",
    "        save_weights_only=True,\n",
    "        log_weights=True,\n",
    "        log_gradients=True,\n",
    "        save_graph=True,\n",
    "    \n",
    "        save_model=True,\n",
    "    \n",
    "        training_data=get_training_dataset(),\n",
    "        validation_data=get_validation_dataset(),\n",
    "        log_evaluation=True,\n",
    "    \n",
    "        compute_flops=wandb_compute_flops,    \n",
    "        \n",
    "        input_type='image',\n",
    "        output_type='label',\n",
    "        \n",
    "        labels=get_training_dataset().class_names,\n",
    "        predictions=15\n",
    "    )            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_wandb_metrics():\n",
    "    return WandbMetricsLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tensorboard():\n",
    "    global location_to_tensorboard\n",
    "    return TensorBoard(\n",
    "        location_to_tensorboard,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_images=True, \n",
    "        write_steps_per_second=True,\n",
    "        update_freq=True, \n",
    "        profile_batch=False,\n",
    "        embeddings_freq=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbacks() -> list:\n",
    "    callback_list: list = list()\n",
    "\n",
    "    global                      \\\n",
    "        wandb_measure_metrics,  \\\n",
    "        wandb_use_callback,     \\\n",
    "        wandb_compute_flops,    \\\n",
    "        is_using_tensorboard\n",
    "\n",
    "    if is_using_tensorboard:\n",
    "        callback_list.append(\n",
    "            setup_tensorboard()\n",
    "        )\n",
    "\n",
    "    if wandb_measure_metrics:\n",
    "        callback_list.append(\n",
    "            setup_wandb_metrics()\n",
    "        )\n",
    "    \n",
    "    if wandb_use_callback:\n",
    "        callback_list.append(\n",
    "            setup_wandb_callback()\n",
    "        )\n",
    "\n",
    "    return callback_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_training_session(\n",
    "    history\n",
    ") -> None:\n",
    "    global training_history\n",
    "\n",
    "    training_history.append(\n",
    "        history.history\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = str('/') + str(\n",
    "    selected_physical_device.name[\n",
    "        len(\n",
    "            '/physical_device:'\n",
    "        )\n",
    "        :\n",
    "        len(\n",
    "            selected_physical_device.name\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "autotune = tensorflow.data.AUTOTUNE\n",
    "training_rotations: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_seed():\n",
    "    global dataset_seed\n",
    "    dataset_seed = generate_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    global training_dataset, validation_dataset, location_of_dataset, validation_split, dataset_seed, height, width, batches\n",
    "\n",
    "    if not(training_dataset is None)        \\\n",
    "       or                                   \\\n",
    "       not(validation_dataset is None):\n",
    "        refresh_seed()\n",
    "\n",
    "    training_dataset, validation_dataset    \\\n",
    "        = image_dataset_from_directory(\n",
    "        location_of_dataset,\n",
    "        validation_split=validation_split,\n",
    "        subset='both',\n",
    "        seed=dataset_seed,\n",
    "        image_size=(\n",
    "            height,\n",
    "            width\n",
    "        ),\n",
    "        batch_size=batches\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /opt/programming/ORM-Model/notebooks/wandb/ wasn't writable, using system temp directory.\n",
      "wandb: WARNING Path /opt/programming/ORM-Model/notebooks/wandb/ wasn't writable, using system temp directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /opt/programming/ORM-Model/notebooks/wandb/ wasn't writable, using system temp directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdesignermadsen\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20230813_222200-ncnwo6k9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/designermadsen/O2RM/runs/ncnwo6k9' target=\"_blank\">grateful-blaze-67</a></strong> to <a href='https://wandb.ai/designermadsen/O2RM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/designermadsen/O2RM' target=\"_blank\">https://wandb.ai/designermadsen/O2RM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/designermadsen/O2RM/runs/ncnwo6k9' target=\"_blank\">https://wandb.ai/designermadsen/O2RM/runs/ncnwo6k9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 22:22:03.853719: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open /opt/models/O2RM: FAILED_PRECONDITION: /opt/models/O2RM; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /tmp/tensorboard/train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10000 files for training.\n",
      "Using 10000 files for validation.\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 22:22:05.724218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-13 22:22:16.501632: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:543] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.8\n",
      "  /usr/local/cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - ETA: 0s - loss: 2.1719 - accuracy: 0.0889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /tmp/tensorboard/validation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 321s 1s/step - loss: 2.1719 - accuracy: 0.0889 - val_loss: 2.1719 - val_accuracy: 0.0845\n",
      "Epoch 2/12\n",
      "200/200 [==============================] - 291s 1s/step - loss: 2.1719 - accuracy: 0.0869 - val_loss: 2.1719 - val_accuracy: 0.0845\n",
      "Epoch 3/12\n",
      "200/200 [==============================] - 294s 1s/step - loss: 2.1719 - accuracy: 0.0874 - val_loss: 2.1719 - val_accuracy: 0.0845\n",
      "Epoch 4/12\n",
      "200/200 [==============================] - 293s 1s/step - loss: 2.1719 - accuracy: 0.0864 - val_loss: 2.1719 - val_accuracy: 0.0854\n",
      "Epoch 5/12\n",
      "200/200 [==============================] - 295s 1s/step - loss: 2.1719 - accuracy: 0.0859 - val_loss: 2.1719 - val_accuracy: 0.0850\n",
      "Epoch 6/12\n",
      "200/200 [==============================] - 298s 1s/step - loss: 2.1719 - accuracy: 0.0869 - val_loss: 2.1719 - val_accuracy: 0.0850\n",
      "Epoch 7/12\n",
      "200/200 [==============================] - 295s 1s/step - loss: 2.1719 - accuracy: 0.0894 - val_loss: 2.1719 - val_accuracy: 0.0850\n",
      "Epoch 8/12\n",
      "200/200 [==============================] - 294s 1s/step - loss: 2.1719 - accuracy: 0.0894 - val_loss: 2.1719 - val_accuracy: 0.0850\n",
      "Epoch 9/12\n",
      "200/200 [==============================] - 300s 1s/step - loss: 2.1719 - accuracy: 0.0884 - val_loss: 2.1719 - val_accuracy: 0.0850\n",
      "Epoch 10/12\n",
      "200/200 [==============================] - 300s 1s/step - loss: 2.1719 - accuracy: 0.0889 - val_loss: 2.1719 - val_accuracy: 0.0850\n",
      "Epoch 11/12\n",
      "200/200 [==============================] - 300s 1s/step - loss: 2.1719 - accuracy: 0.0894 - val_loss: 2.1719 - val_accuracy: 0.0850\n",
      "Epoch 12/12\n",
      "200/200 [==============================] - 304s 2s/step - loss: 2.1719 - accuracy: 0.0908 - val_loss: 2.1719 - val_accuracy: 0.0850\n",
      "INFO:tensorflow:Assets written to: /opt/models/O2RM/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /opt/models/O2RM/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/opt/models/O2RM)... Done. 0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling (Rescaling)       (None, 512, 512, 3)       0         \n",
      "                                                                 \n",
      " new_layer_1 (Conv2D)        (None, 512, 512, 3)       84        \n",
      "                                                                 \n",
      " new_layer_2 (Conv2D)        (None, 512, 512, 3)       84        \n",
      "                                                                 \n",
      " new_layer_3 (Conv2D)        (None, 512, 512, 3)       84        \n",
      "                                                                 \n",
      " new_layer_4 (Conv2D)        (None, 512, 512, 3)       84        \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 512, 512, 2)       56        \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 512, 512, 2)       38        \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 512, 512, 2)       38        \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 512, 512, 2)       38        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 256, 256, 2)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 256, 256, 4)       76        \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 256, 256, 4)       148       \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 256, 256, 4)       148       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 256, 256, 4)       148       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 128, 128, 4)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 128, 128, 8)       296       \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 128, 128, 8)       584       \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 128, 128, 8)       584       \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 128, 128, 8)       584       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 64, 64, 8)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 64, 64, 16)        1168      \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 64, 64, 16)        2320      \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 64, 64, 16)        2320      \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 64, 64, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 32, 32, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 32, 32, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 16, 16, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 8, 8, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              4195328   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4898124 (18.68 MB)\n",
      "Trainable params: 4898124 (18.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    entity=wandb_entity,\n",
    "    project=wandb_project,\n",
    "    config=configuration,\n",
    "    tags=wandb_tags,\n",
    "    job_type=wandb_jobtype,\n",
    "    reinit=True,\n",
    "    tensorboard=is_using_tensorboard,\n",
    "    save_code=False,\n",
    "    notes=\"\"\n",
    ")\n",
    "\n",
    "with tensorflow.device(\n",
    "    device_name\n",
    "):\n",
    "    load_old_model_weights()\n",
    "    load_dataset()\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            'seed': dataset_seed,\n",
    "            'training': {\n",
    "                'labels': get_training_dataset().class_names\n",
    "            },\n",
    "            'validation': {\n",
    "                'labels': get_validation_dataset().class_names\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        training_dataset.prefetch(\n",
    "            buffer_size=autotune\n",
    "        ),\n",
    "        \n",
    "        validation_data=validation_dataset.prefetch(\n",
    "            buffer_size=autotune\n",
    "        ),\n",
    "\n",
    "        epochs=epochs,\n",
    "\n",
    "        callbacks=callbacks(),\n",
    "\n",
    "        use_multiprocessing=use_multiprocessing,\n",
    "        workers=process_workers\n",
    "    )\n",
    "\n",
    "    append_training_session(\n",
    "        history\n",
    "    )\n",
    "\n",
    "    model.save(\n",
    "        location_of_model,\n",
    "        save_format='tf',\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    saved_model = wandb.Artifact(\n",
    "        \"o2rm_model\",\n",
    "        type=\"model\"\n",
    "    )\n",
    "\n",
    "    saved_model.add_dir(\n",
    "        location_of_model\n",
    "    )\n",
    "\n",
    "    wandb.log_artifact(\n",
    "        saved_model\n",
    "    )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 10 classes.\n",
      "Using 10000 files for training.\n",
      "Using 10000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 74s 370ms/step - loss: 2.1719 - accuracy: 0.0898\n",
      "200/200 [==============================] - 79s 393ms/step - loss: 2.1719 - accuracy: 0.0874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.171875, 0.08740234375]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(\n",
    "    training_dataset.prefetch(\n",
    "        buffer_size=autotune\n",
    "    ),\n",
    "    callbacks=[WandbMetricsLogger()]\n",
    ")\n",
    "\n",
    "model.evaluate(\n",
    "    validation_dataset.prefetch(\n",
    "        buffer_size=autotune\n",
    "    ),\n",
    "    callbacks=[WandbMetricsLogger()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▅▂▃▂▁▂▆▆▅▅▆█</td></tr><tr><td>epoch/epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▁▁█▅▅▅▅▅▅▅▅</td></tr><tr><td>epoch/val_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>seed</td><td>▁</td></tr><tr><td>train/batch_accuracy</td><td>▄▅▃▂▅▃▁▄▄▂▂▄▂▁▅▃▁▅▃▂▄▆▃▄▆▃▂▆▃▂█▅▂▄▆▃▃▆▃▂</td></tr><tr><td>train/batch_loss</td><td>▄▇▄▁▇▄▂▆▅▂▄█▃▁▇▄▁▇▄▂▄█▃▄█▃▁▇▄▁▄▇▃▄█▃▄█▃▁</td></tr><tr><td>train/batch_steps_per_second</td><td>█████▇▇▆▇█▇▄▅▇▆▅▆▆▆▅▇▆▆█▆▇▇▁▆▆▇▆▆▇▄▆▇▇▇▅</td></tr><tr><td>train/epoch_accuracy</td><td>▅▂▃▂▁▂▆▆▅▅▆█</td></tr><tr><td>train/epoch_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch_steps_per_second</td><td>▁█▇▇▇▆▇▇▆▆▆▅</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>validation/epoch_accuracy</td><td>▁▁▁█▅▅▅▅▅▅▅▅</td></tr><tr><td>validation/epoch_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation/evaluation_accuracy_vs_iterations</td><td>▁▁▁█▅▅▅▅▅▅▅▅</td></tr><tr><td>validation/evaluation_loss_vs_iterations</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation/global_step</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.09082</td></tr><tr><td>epoch/epoch</td><td>11</td></tr><tr><td>epoch/learning_rate</td><td>0.00132</td></tr><tr><td>epoch/loss</td><td>2.17188</td></tr><tr><td>epoch/val_accuracy</td><td>0.08496</td></tr><tr><td>epoch/val_loss</td><td>2.17188</td></tr><tr><td>global_step</td><td>6156</td></tr><tr><td>seed</td><td>15993</td></tr><tr><td>train/batch_accuracy</td><td>0.09082</td></tr><tr><td>train/batch_loss</td><td>2.17188</td></tr><tr><td>train/batch_steps_per_second</td><td>0.90946</td></tr><tr><td>train/epoch_accuracy</td><td>0.09082</td></tr><tr><td>train/epoch_loss</td><td>2.17188</td></tr><tr><td>train/epoch_steps_per_second</td><td>0.65792</td></tr><tr><td>train/global_step</td><td>11</td></tr><tr><td>validation/epoch_accuracy</td><td>0.08496</td></tr><tr><td>validation/epoch_loss</td><td>2.17188</td></tr><tr><td>validation/evaluation_accuracy_vs_iterations</td><td>0.08496</td></tr><tr><td>validation/evaluation_loss_vs_iterations</td><td>2.17188</td></tr><tr><td>validation/global_step</td><td>11</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-blaze-67</strong> at: <a href='https://wandb.ai/designermadsen/O2RM/runs/ncnwo6k9' target=\"_blank\">https://wandb.ai/designermadsen/O2RM/runs/ncnwo6k9</a><br/>Synced 6 W&B file(s), 0 media file(s), 5 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/tmp/wandb/run-20230813_222200-ncnwo6k9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_ops_v2.flush()\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
