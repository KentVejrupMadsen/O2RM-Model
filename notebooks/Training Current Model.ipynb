{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy -q\n",
    "%pip install keras -q\n",
    "\n",
    "%pip install tensorflow -q\n",
    "%pip install tensorboard_plugin_profile -q\n",
    "\n",
    "%pip install networkx -q\n",
    "\n",
    "%pip install wandb -q\n",
    "%pip install kaggle -q\n",
    "\n",
    "%pip install matplotlib -q\n",
    "%pip install ipympl -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 14:51:19.942518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from os.path    \\\n",
    "    import      \\\n",
    "    isdir,      \\\n",
    "    join\n",
    "\n",
    "from os         \\\n",
    "    import      \\\n",
    "    walk,       \\\n",
    "    remove\n",
    "\n",
    "from random                     \\\n",
    "    import                      \\\n",
    "    SystemRandom\n",
    "\n",
    "import wandb\n",
    "\n",
    "from wandb.keras                \\\n",
    "    import                      \\\n",
    "    WandbMetricsLogger,         \\\n",
    "    WandbCallback\n",
    "\n",
    "from keras.utils                \\\n",
    "    import                      \\\n",
    "    image_dataset_from_directory\n",
    "\n",
    "from keras.backend              \\\n",
    "    import                      \\\n",
    "    clear_session\n",
    "\n",
    "from keras.models               \\\n",
    "    import                      \\\n",
    "    load_model\n",
    "\n",
    "from keras.optimizers           \\\n",
    "    import SGD\n",
    "\n",
    "from keras.callbacks            \\\n",
    "    import TensorBoard\n",
    "\n",
    "from keras                      \\\n",
    "    import mixed_precision\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.python.ops      \\\n",
    "  import summary_ops_v2\n",
    "\n",
    "from keras.losses \\\n",
    "    import SparseCategoricalCrossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_using_tensorboard: bool = True\n",
    "location_to_tensorboard: str = '/tmp/tensorboard'\n",
    "\n",
    "if is_using_tensorboard:\n",
    "    if isdir(\n",
    "        location_to_tensorboard\n",
    "    ):\n",
    "        for root,           \\\n",
    "            directories,    \\\n",
    "            files           \\\n",
    "                in walk(\n",
    "                    location_to_tensorboard\n",
    "                ):\n",
    "            \n",
    "            for file in files:\n",
    "                full_path_to_file: str = join(\n",
    "                    root, \n",
    "                    file\n",
    "                )\n",
    "\n",
    "                remove(\n",
    "                    full_path_to_file\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_ipympl: bool = False\n",
    "\n",
    "if use_ipympl:\n",
    "    %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_policy = mixed_precision.Policy(\n",
    "    'float32'\n",
    ")\n",
    "\n",
    "mixed_precision.set_global_policy(\n",
    "    global_policy\n",
    ")\n",
    "\n",
    "tensorflow.keras                                        \\\n",
    "    .mixed_precision                                    \\\n",
    "    .set_global_policy(\n",
    "    global_policy\n",
    ")\n",
    "\n",
    "gpu_memory_growth: bool = True\n",
    "\n",
    "def zero() -> int:\n",
    "    return 0\n",
    "\n",
    "\n",
    "physical_devices = tensorflow.config.list_physical_devices(\n",
    "    str(\n",
    "        'gpu'\n",
    "    ).upper()\n",
    ")\n",
    "\n",
    "selected_physical_device = physical_devices[\n",
    "    zero()\n",
    "]\n",
    "\n",
    "tensorflow.config.experimental.set_memory_growth(\n",
    "    selected_physical_device,\n",
    "    gpu_memory_growth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_of_model: str = '/opt/models/O2RM'\n",
    "\n",
    "def get_location_of_model() -> str:\n",
    "    global location_of_model\n",
    "    return location_of_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_old_model_weights():\n",
    "    global          \\\n",
    "        model,      \\\n",
    "        location_of_model\n",
    "\n",
    "    if isdir(\n",
    "        location_of_model\n",
    "    ):\n",
    "        model.load_weights(\n",
    "            location_of_model\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "model = load_model(\n",
    "    location_of_model\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=SGD(\n",
    "        learning_rate=0.00024\n",
    "    ),\n",
    "    loss=SparseCategoricalCrossentropy(\n",
    "        from_logits=True\n",
    "    ),\n",
    "    metrics=[\n",
    "        'accuracy'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seed():\n",
    "    return SystemRandom().randint(\n",
    "        1,\n",
    "        32767\n",
    "    )\n",
    "\n",
    "def refresh_seed():\n",
    "    global dataset_seed\n",
    "    dataset_seed = generate_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history: list = list()\n",
    "\n",
    "training_dataset = None\n",
    "validation_dataset = None\n",
    "\n",
    "dataset_seed: int = generate_seed()\n",
    "\n",
    "location_of_dataset: str = '/opt/dataset/numbers'\n",
    "\n",
    "width: int = 512\n",
    "height: int = 512\n",
    "channels: int = 3\n",
    "\n",
    "number_of_labels: int = 10\n",
    "\n",
    "batches: int = 60\n",
    "epochs: int = 12\n",
    "\n",
    "validation_split: float = 0.15\n",
    "\n",
    "use_multiprocessing: bool = True\n",
    "process_workers: int = 8\n",
    "\n",
    "tensorflow_verbose: int = 1\n",
    "\n",
    "training_labels: list = list()\n",
    "validation_labels: list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_entity: str = 'designermadsen'\n",
    "wandb_project: str = 'O2RM'\n",
    "\n",
    "wandb_jobtype: str = 'Training'\n",
    "\n",
    "wandb_tags: list = [\n",
    "    'Nvidia',\n",
    "    'Linux',\n",
    "    'Ubuntu',\n",
    "    'Development',\n",
    "    'Random',\n",
    "    'test-driven',\n",
    "    'Bare-Metal',\n",
    "    'TensorFlow'   \n",
    "]\n",
    "\n",
    "wandb_use_callback: bool = False\n",
    "wandb_measure_metrics: bool = True\n",
    "wandb_compute_flops: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration: dict = {\n",
    "    'vision': \n",
    "    {\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'channels': channels\n",
    "    },\n",
    "\n",
    "    'dataset': \n",
    "    {\n",
    "        'number of labels': number_of_labels,\n",
    "        'batches': batches,\n",
    "        'epochs': epochs,\n",
    "        'seed': dataset_seed,\n",
    "        'using multiprocessing': use_multiprocessing,\n",
    "        'using tensorboard': is_using_tensorboard,\n",
    "        'workers': process_workers,\n",
    "    },\n",
    "\n",
    "    'wandb':\n",
    "    {\n",
    "        'job type': wandb_jobtype,\n",
    "        'tags': wandb_tags,\n",
    "        'measure metrics': wandb_measure_metrics,\n",
    "        'use callbacks': wandb_use_callback,\n",
    "        'compute flops': wandb_compute_flops\n",
    "    },\n",
    "\n",
    "    'gpu':\n",
    "    {\n",
    "        'memory growth': gpu_memory_growth\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset():\n",
    "    global training_dataset\n",
    "    return training_dataset\n",
    "\n",
    "def get_validation_dataset():\n",
    "    global validation_dataset\n",
    "    return validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_wandb_callback():\n",
    "    global wandb_compute_flops\n",
    "\n",
    "    return WandbCallback(\n",
    "        monitor='val_loss', \n",
    "        verbose=0,\n",
    "    \n",
    "        save_weights_only=True,\n",
    "        log_weights=True,\n",
    "        log_gradients=True,\n",
    "\n",
    "        save_graph=True,\n",
    "        save_model=True,\n",
    "    \n",
    "        training_data=get_training_dataset(),\n",
    "        validation_data=get_validation_dataset(),\n",
    "        log_evaluation=True,\n",
    "    \n",
    "        compute_flops=wandb_compute_flops,    \n",
    "        \n",
    "        input_type='image',\n",
    "        output_type='label',\n",
    "        \n",
    "        labels=get_training_dataset().class_names,\n",
    "        predictions=15\n",
    "    )            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_tensorboard():\n",
    "    global location_to_tensorboard\n",
    "    return TensorBoard(\n",
    "        location_to_tensorboard,\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_images=True, \n",
    "        write_steps_per_second=True,\n",
    "        update_freq=True, \n",
    "        profile_batch=False,\n",
    "        embeddings_freq=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_wandb_metrics():\n",
    "    return WandbMetricsLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbacks() -> list:\n",
    "    global                      \\\n",
    "        wandb_measure_metrics,  \\\n",
    "        wandb_use_callback,     \\\n",
    "        wandb_compute_flops,    \\\n",
    "        is_using_tensorboard\n",
    "\n",
    "    callback_list: list = list()\n",
    "\n",
    "    if is_using_tensorboard:\n",
    "        callback_list.append(\n",
    "            setup_tensorboard()\n",
    "        )\n",
    "\n",
    "    if wandb_measure_metrics:\n",
    "        callback_list.append(\n",
    "            setup_wandb_metrics()\n",
    "        )\n",
    "    \n",
    "    if wandb_use_callback:\n",
    "        callback_list.append(\n",
    "            setup_wandb_callback()\n",
    "        )\n",
    "\n",
    "    return callback_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_training_session(\n",
    "    history\n",
    ") -> None:\n",
    "    global training_history\n",
    "\n",
    "    training_history.append(\n",
    "        history.history\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = str('/') + str(\n",
    "    selected_physical_device.name[\n",
    "        len(\n",
    "            '/physical_device:'\n",
    "        )\n",
    "        :\n",
    "        len(\n",
    "            selected_physical_device.name\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "autotune = tensorflow.data.AUTOTUNE\n",
    "training_rotations: int = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_training_dataset_none() -> bool:\n",
    "    global training_dataset\n",
    "    return training_dataset is None\n",
    "\n",
    "def is_validation_dataset_none() -> bool:\n",
    "    global validation_dataset\n",
    "    return validation_dataset is None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers               \\\n",
    "    import                      \\\n",
    "    RandomFlip,                 \\\n",
    "    RandomZoom,                 \\\n",
    "    RandomContrast,             \\\n",
    "    RandomBrightness,           \\\n",
    "    RandomRotation,             \\\n",
    "    RandomTranslation,          \\\n",
    "    RandomHeight,               \\\n",
    "    RandomWidth,                \\\n",
    "    RandomCrop                  \n",
    "\n",
    "def augmentation_layers() -> Sequential:\n",
    "    layers: list = list()\n",
    "\n",
    "    layers.append(\n",
    "        RandomFlip(\n",
    "            'horizontal_and_vertical',\n",
    "            seed=generate_seed()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layers.append(\n",
    "        RandomRotation(\n",
    "            factor=(\n",
    "                -1.0, \n",
    "                1.0\n",
    "            ),\n",
    "            fill_mode='nearest',\n",
    "            seed=generate_seed()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layers.append(\n",
    "        RandomZoom(\n",
    "            height_factor=(\n",
    "                -1.0, \n",
    "                1.0\n",
    "            ),\n",
    "            width_factor=(\n",
    "                -1.0, \n",
    "                1.0\n",
    "            ),\n",
    "            fill_mode='nearest',\n",
    "            seed=generate_seed()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layers.append(\n",
    "        RandomContrast(\n",
    "            factor=1.0,\n",
    "            seed=generate_seed()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    layers.append(\n",
    "        RandomBrightness(\n",
    "            factor=1.0,\n",
    "            seed=generate_seed()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    augmentation_layers: Sequential = Sequential(\n",
    "        layers\n",
    "    )\n",
    "\n",
    "    return augmentation_layers\n",
    "\n",
    "def data_augmentation():\n",
    "    global training_dataset, validation_dataset, autotune\n",
    "    \n",
    "    training_augmentation = augmentation_layers()\n",
    "    training_dataset = training_dataset.map(\n",
    "        lambda x, y: (\n",
    "            training_augmentation(x, training=True), y\n",
    "        ), \n",
    "        num_parallel_calls=autotune\n",
    "    )\n",
    "\n",
    "    validation_augmentation = augmentation_layers()\n",
    "    validation_dataset = validation_dataset.map(\n",
    "        lambda x, y: (\n",
    "            validation_augmentation(x, training=True), y\n",
    "        ), \n",
    "        num_parallel_calls=autotune\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    global                      \\\n",
    "        training_dataset,       \\\n",
    "        validation_dataset,     \\\n",
    "        location_of_dataset,    \\\n",
    "        validation_split,       \\\n",
    "        dataset_seed,           \\\n",
    "        height,                 \\\n",
    "        width,                  \\\n",
    "        batches,                \\\n",
    "        training_labels,        \\\n",
    "        validation_labels\n",
    "\n",
    "    if(\n",
    "        not(\n",
    "            is_training_dataset_none()\n",
    "        )      \n",
    "        or                                   \n",
    "        not(\n",
    "            is_validation_dataset_none()\n",
    "        )\n",
    "    ):\n",
    "        refresh_seed()\n",
    "\n",
    "    training_dataset, validation_dataset    \\\n",
    "        = image_dataset_from_directory(\n",
    "        location_of_dataset,\n",
    "        validation_split=validation_split,\n",
    "        subset='both',\n",
    "        seed=dataset_seed,\n",
    "        image_size=(\n",
    "            height,\n",
    "            width\n",
    "        ),\n",
    "        batch_size=batches\n",
    "    )\n",
    "\n",
    "    training_labels = training_dataset.class_names\n",
    "    validation_labels = validation_dataset.class_names\n",
    "\n",
    "    data_augmentation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    global training_labels, validation_labels\n",
    "    load_old_model_weights()\n",
    "    load_dataset()\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            'seed': dataset_seed,\n",
    "            'training': {\n",
    "                'labels': training_labels\n",
    "            },\n",
    "            'validation': {\n",
    "                'labels': validation_labels\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        training_dataset.prefetch(\n",
    "            buffer_size=autotune\n",
    "        ),\n",
    "        \n",
    "        validation_data=validation_dataset.prefetch(\n",
    "            buffer_size=autotune\n",
    "        ),\n",
    "\n",
    "        epochs=epochs,\n",
    "\n",
    "        callbacks=callbacks(),\n",
    "\n",
    "        use_multiprocessing=use_multiprocessing,\n",
    "        workers=process_workers\n",
    "    )\n",
    "\n",
    "    append_training_session(\n",
    "        history\n",
    "    )\n",
    "\n",
    "    model.save(\n",
    "        location_of_model,\n",
    "        save_format='tf',\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    saved_model = wandb.Artifact(\n",
    "        \"o2rm_model\",\n",
    "        type=\"model\"\n",
    "    )\n",
    "\n",
    "    saved_model.add_dir(\n",
    "        location_of_model\n",
    "    )\n",
    "\n",
    "    wandb.log_artifact(\n",
    "        saved_model\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation():\n",
    "    global model, training_dataset, validation_dataset\n",
    "    load_dataset()\n",
    "\n",
    "    model.evaluate(\n",
    "        training_dataset.prefetch(\n",
    "            buffer_size=autotune\n",
    "        ),\n",
    "        callbacks=[\n",
    "            WandbMetricsLogger()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.evaluate(\n",
    "        validation_dataset.prefetch(\n",
    "            buffer_size=autotune\n",
    "        ),\n",
    "        callbacks=[\n",
    "            WandbMetricsLogger()\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    global                      \\\n",
    "        wandb_entity,           \\\n",
    "        wandb_project,          \\\n",
    "        configuration,          \\\n",
    "        wandb_tags,             \\\n",
    "        wandb_jobtype,          \\\n",
    "        is_using_tensorboard\n",
    "\n",
    "    wandb.init(\n",
    "        entity=wandb_entity,\n",
    "        project=wandb_project,\n",
    "        config=configuration,\n",
    "        tags=wandb_tags,\n",
    "        job_type=wandb_jobtype,\n",
    "        reinit=True,\n",
    "        tensorboard=is_using_tensorboard,\n",
    "        save_code=True,\n",
    "        notes=\"\",\n",
    "        magic=True\n",
    "    )\n",
    "\n",
    "def shutdown():\n",
    "    summary_ops_v2.flush()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /opt/programming/ORM-Model/notebooks/wandb/ wasn't writable, using system temp directory.\n",
      "wandb: WARNING Path /opt/programming/ORM-Model/notebooks/wandb/ wasn't writable, using system temp directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /opt/programming/ORM-Model/notebooks/wandb/ wasn't writable, using system temp directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdesignermadsen\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m wandb.init() arguments ignored because wandb magic has already been initialized\n",
      "warning: in the working copy of 'src/O2RMModel/RecognitionModel/Training/momentum_decay.py', CRLF will be replaced by LF the next time Git touches it\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20230814_145507-rm7n7uxv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/designermadsen/O2RM/runs/rm7n7uxv' target=\"_blank\">worthy-brook-79</a></strong> to <a href='https://wandb.ai/designermadsen/O2RM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/designermadsen/O2RM' target=\"_blank\">https://wandb.ai/designermadsen/O2RM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/designermadsen/O2RM/runs/rm7n7uxv' target=\"_blank\">https://wandb.ai/designermadsen/O2RM/runs/rm7n7uxv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rm7n7uxv) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e3594bec9a4154b828b6de283805ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.030 MB of 0.066 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.452287…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worthy-brook-79</strong> at: <a href='https://wandb.ai/designermadsen/O2RM/runs/rm7n7uxv' target=\"_blank\">https://wandb.ai/designermadsen/O2RM/runs/rm7n7uxv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/tmp/wandb/run-20230814_145507-rm7n7uxv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rm7n7uxv). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ab5829fc74437dae84e8ed3d851b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666836570000214, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: in the working copy of 'src/O2RMModel/RecognitionModel/Training/momentum_decay.py', CRLF will be replaced by LF the next time Git touches it\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20230814_145510-5zbrwrxb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/designermadsen/O2RM/runs/5zbrwrxb' target=\"_blank\">comic-bird-80</a></strong> to <a href='https://wandb.ai/designermadsen/O2RM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/designermadsen/O2RM' target=\"_blank\">https://wandb.ai/designermadsen/O2RM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/designermadsen/O2RM/runs/5zbrwrxb' target=\"_blank\">https://wandb.ai/designermadsen/O2RM/runs/5zbrwrxb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 10 classes.\n",
      "Using 17000 files for training.\n",
      "Using 3000 files for validation.\n",
      "284/284 [==============================] - 268s 868ms/step - loss: 2.3029 - accuracy: 0.0994\n",
      "50/50 [==============================] - 51s 886ms/step - loss: 2.3028 - accuracy: 0.1037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 15:00:38.637406: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open /opt/models/O2RM: FAILED_PRECONDITION: /opt/models/O2RM; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 10 classes.\n",
      "Using 17000 files for training.\n",
      "Using 3000 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The data_type argument of wandb.keras.WandbCallback is deprecated and will be removed in a future release. Please use input_type instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Setting input_type = data_type.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /tmp/tensorboard/train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 15:00:42.197016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-14 15:00:52.551202: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:543] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  /usr/local/cuda-11.8\n",
      "  /usr/local/cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
      "2023-08-14 15:01:05.621507: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 736.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/284 [..............................] - ETA: 2:38:08 - loss: 2.3026 - accuracy: 0.1167"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 15:01:17.051347: W tensorflow/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 736.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284/284 [==============================] - ETA: 0s - loss: 2.3034 - accuracy: 0.0946"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Found log directory outside of given root_logdir, dropping given root_logdir for event file in /tmp/tensorboard/validation\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No validation_data set, pass a generator to the callback.\n",
      "/home/vulgrim/miniconda3/envs/tensorflow/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/wandb/run-20230814_145510-5zbrwrxb/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/wandb/run-20230814_145510-5zbrwrxb/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/tmp/wandb/run-20230814_145510-5zbrwrxb/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284/284 [==============================] - 557s 2s/step - loss: 2.3034 - accuracy: 0.0946 - val_loss: 2.3028 - val_accuracy: 0.0947\n",
      "Epoch 2/12\n",
      "284/284 [==============================] - ETA: 0s - loss: 2.3035 - accuracy: 0.0932"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No validation_data set, pass a generator to the callback.\n",
      "/home/vulgrim/miniconda3/envs/tensorflow/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/wandb/run-20230814_145510-5zbrwrxb/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/wandb/run-20230814_145510-5zbrwrxb/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/tmp/wandb/run-20230814_145510-5zbrwrxb/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284/284 [==============================] - 542s 2s/step - loss: 2.3035 - accuracy: 0.0932 - val_loss: 2.3028 - val_accuracy: 0.0947\n",
      "Epoch 3/12\n",
      "284/284 [==============================] - ETA: 0s - loss: 2.3035 - accuracy: 0.0919"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No validation_data set, pass a generator to the callback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284/284 [==============================] - 540s 2s/step - loss: 2.3035 - accuracy: 0.0919 - val_loss: 2.3029 - val_accuracy: 0.0980\n",
      "Epoch 4/12\n",
      "284/284 [==============================] - ETA: 0s - loss: 2.3035 - accuracy: 0.0994"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m No validation_data set, pass a generator to the callback.\n",
      "/home/vulgrim/miniconda3/envs/tensorflow/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/wandb/run-20230814_145510-5zbrwrxb/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/wandb/run-20230814_145510-5zbrwrxb/files/model-best/assets\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/tmp/wandb/run-20230814_145510-5zbrwrxb/files/model-best)... Done. 0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284/284 [==============================] - 577s 2s/step - loss: 2.3035 - accuracy: 0.0994 - val_loss: 2.3026 - val_accuracy: 0.0947\n",
      "Epoch 5/12\n",
      " 26/284 [=>............................] - ETA: 6:01 - loss: 2.3035 - accuracy: 0.0846"
     ]
    }
   ],
   "source": [
    "with tensorflow.device(\n",
    "    device_name\n",
    "):\n",
    "    setup()\n",
    "    evaluation()\n",
    "    training()\n",
    "\n",
    "shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
